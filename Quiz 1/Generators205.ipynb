{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "#imports necessary to define a neural network \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#ensure you are using GPU.\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev)\n",
    "print(device)\n",
    "\n",
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 28395])\n",
      "torch.Size([30, 28395])\n",
      "torch.Size([200])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trdt1=pd.read_csv('C:\\\\Users\\\\Ant Pc\\\\GitHub\\\\pROJ\\\\Genotype\\\\TrainingGenotype.csv')\n",
    "trdt=pd.read_csv('C:\\\\Users\\\\Ant Pc\\\\GitHub\\\\pROJ\\\\Geneexpression0\\\\traingeneexpression.csv',header=None)\n",
    "tstdt=pd.read_csv('C:\\\\Users\\\\Ant Pc\\\\GitHub\\\\pROJ\\\\Geneexpression0\\\\testgeneexpression.csv',header=None)\n",
    "\n",
    "tstdt=np.array(tstdt)\n",
    "tstdt=np.transpose(tstdt)\n",
    "tstdt=pd.DataFrame(tstdt)\n",
    "\n",
    "trdt=np.array(trdt)\n",
    "trdt=np.transpose(trdt)\n",
    "trdt=pd.DataFrame(trdt)\n",
    "\n",
    "results1=trdt1['Phenotype1']\n",
    "results2=trdt1['Phenotype2']\n",
    "# convert matrices to pytorch tensors on gpu\n",
    "\n",
    "trdttensor=torch.from_numpy(trdt.values).type(dtype)\n",
    "tstdttensor=torch.from_numpy(tstdt.values).type(dtype)\n",
    "results1tensor=torch.from_numpy(results1.values).type(dtype)\n",
    "results2tensor=torch.from_numpy(results2.values).type(dtype)\n",
    "\n",
    "print(trdttensor.shape)\n",
    "print(tstdttensor.shape)\n",
    "print(results2tensor.shape)\n",
    "print(results1tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold=pd.read_csv('C:/Users/Ant Pc/GitHub/PROJECT/DATA/DREAM5_SysGenB2_GoldStandard.csv', sep='\\t')\n",
    "gold.drop(gold.columns[-1],axis=1,inplace=True)\n",
    "\n",
    "gold2=gold.iloc[1,:]\n",
    "gold2=gold2.values\n",
    "\n",
    "dt2=np.transpose(gold2)\n",
    "\n",
    "gold1=gold.iloc[0,:]\n",
    "gold1=gold1.values\n",
    "\n",
    "dt1=np.transpose(gold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net1(\n",
      "  (rand): Linear(in_features=28395, out_features=1000, bias=True)\n",
      "  (fc1): Linear(in_features=1000, out_features=3200, bias=True)\n",
      "  (fc2): Linear(in_features=3200, out_features=3200, bias=True)\n",
      "  (fc3): Linear(in_features=3200, out_features=3200, bias=True)\n",
      "  (fc6): Linear(in_features=3200, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "randmatr=pd.read_csv('C:/Users/Ant Pc/GitHub/PROJECT/Randmatr.csv', header=None).values\n",
    "class Net1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        \n",
    "        self.rand=nn.Linear(28395,1000).cuda()\n",
    "        self.fc1=nn.Linear(1000,3200).cuda()\n",
    "        self.fc2=nn.Linear(3200,3200).cuda()\n",
    "        self.fc3=nn.Linear(3200,3200).cuda()\n",
    "        self.fc6 = nn.Linear(3200,1).cuda()\n",
    "        \n",
    "        #set weights as random projection matrix\n",
    "        self.rand.weight.data = torch.transpose(torch.from_numpy(randmatr),0,1).type(dtype)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.rand(x).cuda()\n",
    "        x = F.relu(x).cuda()\n",
    "        x = F.relu(self.fc1(x)).cuda() \n",
    "        x = F.relu(self.fc2(x)).cuda()   \n",
    "        x = F.relu(self.fc3(x)).cuda()      \n",
    "        x = self.fc6(x).cuda()\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Net1()\n",
    "\n",
    "#use gpu for all computations in model\n",
    "model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.2362, device='cuda:0', grad_fn=<L1LossBackward>) 0\n",
      "tensor(6.6995, device='cuda:0', grad_fn=<L1LossBackward>) 1\n",
      "tensor(3.8907, device='cuda:0', grad_fn=<L1LossBackward>) 2\n",
      "tensor(2.5916, device='cuda:0', grad_fn=<L1LossBackward>) 3\n",
      "tensor(2.5141, device='cuda:0', grad_fn=<L1LossBackward>) 4\n",
      "tensor(2.7419, device='cuda:0', grad_fn=<L1LossBackward>) 5\n",
      "tensor(3.1076, device='cuda:0', grad_fn=<L1LossBackward>) 6\n",
      "tensor(5.6500, device='cuda:0', grad_fn=<L1LossBackward>) 7\n",
      "tensor(5.6306, device='cuda:0', grad_fn=<L1LossBackward>) 8\n",
      "tensor(5.8528, device='cuda:0', grad_fn=<L1LossBackward>) 9\n",
      "tensor(5.9091, device='cuda:0', grad_fn=<L1LossBackward>) 10\n",
      "tensor(5.9468, device='cuda:0', grad_fn=<L1LossBackward>) 11\n",
      "tensor(5.9385, device='cuda:0', grad_fn=<L1LossBackward>) 12\n",
      "tensor(5.8785, device='cuda:0', grad_fn=<L1LossBackward>) 13\n",
      "tensor(5.7904, device='cuda:0', grad_fn=<L1LossBackward>) 14\n",
      "tensor(5.6993, device='cuda:0', grad_fn=<L1LossBackward>) 15\n",
      "tensor(5.6115, device='cuda:0', grad_fn=<L1LossBackward>) 16\n",
      "tensor(5.4757, device='cuda:0', grad_fn=<L1LossBackward>) 17\n",
      "tensor(5.3507, device='cuda:0', grad_fn=<L1LossBackward>) 18\n",
      "tensor(5.3330, device='cuda:0', grad_fn=<L1LossBackward>) 19\n",
      "tensor(5.1942, device='cuda:0', grad_fn=<L1LossBackward>) 20\n",
      "tensor(5.0464, device='cuda:0', grad_fn=<L1LossBackward>) 21\n",
      "tensor(4.8981, device='cuda:0', grad_fn=<L1LossBackward>) 22\n",
      "tensor(4.7689, device='cuda:0', grad_fn=<L1LossBackward>) 23\n",
      "tensor(4.6298, device='cuda:0', grad_fn=<L1LossBackward>) 24\n",
      "tensor(4.4983, device='cuda:0', grad_fn=<L1LossBackward>) 25\n",
      "tensor(4.3151, device='cuda:0', grad_fn=<L1LossBackward>) 26\n",
      "tensor(4.0882, device='cuda:0', grad_fn=<L1LossBackward>) 27\n",
      "tensor(2.5091, device='cuda:0', grad_fn=<L1LossBackward>) 28\n",
      "tensor(2.5031, device='cuda:0', grad_fn=<L1LossBackward>) 29\n",
      "tensor(2.5040, device='cuda:0', grad_fn=<L1LossBackward>) 30\n",
      "tensor(2.5035, device='cuda:0', grad_fn=<L1LossBackward>) 31\n",
      "tensor(2.5736, device='cuda:0', grad_fn=<L1LossBackward>) 32\n",
      "tensor(2.5544, device='cuda:0', grad_fn=<L1LossBackward>) 33\n",
      "tensor(2.5342, device='cuda:0', grad_fn=<L1LossBackward>) 34\n",
      "tensor(2.5208, device='cuda:0', grad_fn=<L1LossBackward>) 35\n",
      "tensor(2.5268, device='cuda:0', grad_fn=<L1LossBackward>) 36\n",
      "tensor(2.5574, device='cuda:0', grad_fn=<L1LossBackward>) 37\n",
      "tensor(2.6799, device='cuda:0', grad_fn=<L1LossBackward>) 38\n",
      "tensor(2.6516, device='cuda:0', grad_fn=<L1LossBackward>) 39\n",
      "tensor(2.6266, device='cuda:0', grad_fn=<L1LossBackward>) 40\n",
      "tensor(2.6001, device='cuda:0', grad_fn=<L1LossBackward>) 41\n",
      "tensor(2.5699, device='cuda:0', grad_fn=<L1LossBackward>) 42\n",
      "tensor(2.5413, device='cuda:0', grad_fn=<L1LossBackward>) 43\n",
      "tensor(2.5579, device='cuda:0', grad_fn=<L1LossBackward>) 44\n",
      "tensor(5.0844, device='cuda:0', grad_fn=<L1LossBackward>) 45\n",
      "tensor(4.5927, device='cuda:0', grad_fn=<L1LossBackward>) 46\n",
      "tensor(3.8192, device='cuda:0', grad_fn=<L1LossBackward>) 47\n",
      "tensor(2.7343, device='cuda:0', grad_fn=<L1LossBackward>) 48\n",
      "tensor(2.6748, device='cuda:0', grad_fn=<L1LossBackward>) 49\n"
     ]
    }
   ],
   "source": [
    "#trainingloop\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "batchsize=20\n",
    "\n",
    "batches=len(trdttensor)/batchsize\n",
    "\n",
    "epochs=50\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adagrad(model.parameters(), lr)\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for j in range(int(batches)):\n",
    "        \n",
    "        #forward pass\n",
    "        out=model(trdttensor[j:j+batchsize,:].type(dtype))\n",
    "\n",
    "        #compute loss\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(torch.reshape(out,[20]).type(dtype),torch.reshape(results2tensor[j:j+batchsize],[20]).type(dtype)).type(dtype)\n",
    "\n",
    "\n",
    "        #backprop loss i.e. find dloss/dparam for each parameter and store.\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        #clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        #use optimiser to update\n",
    "        optimizer.step()\n",
    "    c=nn.L1Loss()\n",
    "    print(c(torch.reshape(model(tstdttensor),[30]),torch.from_numpy(np.array(dt2)).type(dtype)),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19.3808],\n",
       "        [23.3114],\n",
       "        [20.4759],\n",
       "        [21.9783],\n",
       "        [18.6246],\n",
       "        [20.9809],\n",
       "        [20.5792],\n",
       "        [19.0692],\n",
       "        [21.4399],\n",
       "        [20.6262],\n",
       "        [20.0444],\n",
       "        [19.0925],\n",
       "        [22.4191],\n",
       "        [27.2825],\n",
       "        [18.9363],\n",
       "        [21.3597],\n",
       "        [21.1166],\n",
       "        [18.7511],\n",
       "        [21.1818],\n",
       "        [18.5806]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(trdttensor[0:20,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OGGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1122, -0.8336, -0.8104, -1.0369, -0.6032,  0.8279],\n",
      "        [-0.0751, -0.8436, -0.4878,  0.2767, -1.7381,  1.8533],\n",
      "        [ 0.2180, -0.3195,  1.4491, -0.3874, -0.7247,  0.3371],\n",
      "        ...,\n",
      "        [-0.1126,  0.5114, -0.2978, -0.3862,  0.0341, -1.5288],\n",
      "        [-0.2055,  2.4584,  1.6704,  1.4604,  1.7337, -1.9728],\n",
      "        [ 2.0211, -0.1911,  0.4957, -0.5623, -1.5291,  0.1290]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#random input matrix\n",
    "nsamples=3000\n",
    "X=np.random.randn(nsamples,6)\n",
    "\n",
    "Xtensor=torch.from_numpy(X).type(dtype)\n",
    "print(Xtensor[:,0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customloss(matr):\n",
    "    #input is tensor of size(1,28395)\n",
    "    \n",
    "    inpu=matr\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    pred=model(matr).type(dtype)\n",
    "    ideal=5*torch.ones_like(pred).type(dtype)\n",
    "    loss=criterion(pred,ideal).type(dtype)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (inp): Linear(in_features=6, out_features=3200, bias=True)\n",
      "  (fc1): Linear(in_features=3200, out_features=3200, bias=True)\n",
      "  (fc2): Linear(in_features=3200, out_features=28395, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Neural network to generate geneexpression values\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.inp =nn.Linear(6,3200).cuda()\n",
    "        self.fc1=nn.Linear(3200,3200).cuda()\n",
    "        self.fc2 = nn.Linear(3200,28395).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.inp(x)).cuda()\n",
    "        x = self.fc1(x).cuda()\n",
    "        x = self.fc2(x).cuda()\n",
    "        x=torch.abs(x).cuda()\n",
    "        x[x>10]=x[x>10]/10\n",
    "        x[x<1]=x[x<1]*100\n",
    "        x=x.cuda()\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20977.6543, device='cuda:0', grad_fn=<MseLossBackward>) 0\n",
      "tensor(53.8950, device='cuda:0', grad_fn=<MseLossBackward>) 1\n",
      "tensor(685.9599, device='cuda:0', grad_fn=<MseLossBackward>) 2\n",
      "tensor(3608.3394, device='cuda:0', grad_fn=<MseLossBackward>) 3\n",
      "tensor(45.3038, device='cuda:0', grad_fn=<MseLossBackward>) 4\n",
      "tensor(9.6268, device='cuda:0', grad_fn=<MseLossBackward>) 5\n",
      "tensor(6.3959, device='cuda:0', grad_fn=<MseLossBackward>) 6\n",
      "tensor(189.8401, device='cuda:0', grad_fn=<MseLossBackward>) 7\n",
      "tensor(143.8048, device='cuda:0', grad_fn=<MseLossBackward>) 8\n",
      "tensor(7.5007, device='cuda:0', grad_fn=<MseLossBackward>) 9\n",
      "tensor(7.4925, device='cuda:0', grad_fn=<MseLossBackward>) 10\n",
      "tensor(3.3244, device='cuda:0', grad_fn=<MseLossBackward>) 11\n",
      "tensor(4.2360, device='cuda:0', grad_fn=<MseLossBackward>) 12\n",
      "tensor(2.3196, device='cuda:0', grad_fn=<MseLossBackward>) 13\n",
      "tensor(2.8710, device='cuda:0', grad_fn=<MseLossBackward>) 14\n",
      "tensor(2.6677, device='cuda:0', grad_fn=<MseLossBackward>) 15\n",
      "tensor(2.4760, device='cuda:0', grad_fn=<MseLossBackward>) 16\n",
      "tensor(1.5486, device='cuda:0', grad_fn=<MseLossBackward>) 17\n",
      "tensor(38.6764, device='cuda:0', grad_fn=<MseLossBackward>) 18\n",
      "tensor(22.8712, device='cuda:0', grad_fn=<MseLossBackward>) 19\n"
     ]
    }
   ],
   "source": [
    "#trainingloop\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "batchsize=100\n",
    "\n",
    "batches=nsamples/batchsize\n",
    "\n",
    "epochs=20\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adagrad(net.parameters(), lr)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for j in range(int(batches)):\n",
    "        \n",
    "        #forward pass\n",
    "        out=net(Xtensor[j:j+batchsize,:]).type(dtype)\n",
    "\n",
    "        #compute loss\n",
    "        loss = customloss(out).type(dtype)\n",
    "\n",
    "\n",
    "        #backprop loss i.e. find dloss/dparam for each parameter and store.\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        #clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    print(loss,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15.1280, 14.5481, 17.3364,  ..., 17.1182,  4.1342, 39.2478],\n",
       "        [13.0939,  0.7497,  4.8362,  ..., 14.2533, 11.8087, 19.2751],\n",
       "        [ 1.4948,  4.3236,  7.4085,  ..., 11.1555,  5.1018,  8.9809],\n",
       "        [23.4049, 16.7881,  3.2906,  ...,  5.9219,  2.5359,  1.1345]],\n",
       "       device='cuda:0', grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(Xtensor[0:4,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7759, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customloss(net(Xtensor[4,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2062],\n",
       "        [6.9819],\n",
       "        [4.1511],\n",
       "        [5.6062],\n",
       "        [3.6674],\n",
       "        [8.0124],\n",
       "        [3.7984],\n",
       "        [5.7121],\n",
       "        [3.5844],\n",
       "        [4.5113],\n",
       "        [5.7482],\n",
       "        [4.3238],\n",
       "        [2.4769],\n",
       "        [4.6265],\n",
       "        [6.9008],\n",
       "        [4.1227],\n",
       "        [3.1758],\n",
       "        [4.5578],\n",
       "        [4.6578],\n",
       "        [5.9837]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(net(Xtensor[0:20,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preddata=net(Xtensor).detach().cpu().numpy()\n",
    "preddata=pd.DataFrame(preddata)\n",
    "preddata=preddata.mean()\n",
    "preddata.to_csv('d0.csv')\n",
    "print((preddata.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
